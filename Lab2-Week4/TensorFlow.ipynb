{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this TensorFlow business?\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, TensorFlow (or PyTorch, if you switch over to that notebook)\n",
    "\n",
    "#### What is it?\n",
    "TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropogation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n",
    "\n",
    "#### Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants, e.g., TensorFlow. This very excellent framework will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement: This exercise is adapted from [Stanford CS231n](http://cs231n.stanford.edu/index.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will I learn TensorFlow?\n",
    "\n",
    "TensorFlow has many excellent tutorials available, including those from [Google themselves](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in TensorFlow. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from libs.tf_layers import *\n",
    "from libs.vis_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from libs.data_utils import load_CIFAR10\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'libs/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Model\n",
    "\n",
    "### Some useful utilities\n",
    "\n",
    ". Remember that our image data is initially N x H x W x C, where:\n",
    "* N is the number of datapoints (mini-batch size)\n",
    "* H is the height of each image in pixels\n",
    "* W is the height of each image in pixels\n",
    "* C is the number of channels (usually 3: R, G, B)\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, which needs spatial understanding of where the pixels are relative to each other. When we input image data into fully connected affine layers, however, we want each data example to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example model itself\n",
    "\n",
    "The first step to training your own model is defining its architecture.\n",
    "\n",
    "Here's an example of a convolutional neural network defined in TensorFlow -- try to understand what each line is doing, remembering that each layer is composed upon the previous layer. We haven't trained anything yet - that'll come next - for now, we want you to understand how everything gets set up. \n",
    "\n",
    "In that example, you see 2D convolutional layers (Conv2d), ReLU activations, and fully-connected layers (Linear). You also see the Hinge loss (multi-class SVM) function, and the SGD optimizer being used. \n",
    "\n",
    "Make sure you understand **why the parameters of the Linear layer are 5408 and 10**. You can refer to the material from [CS231n webpages](http://cs231n.github.io/convolutional-networks/).\n",
    "\n",
    "### TensorFlow Details\n",
    "In TensorFlow, much like in our previous notebooks, we'll first specifically initialize our variables, and then our network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def simple_model(X,y):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[5408, 10])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[10])\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding='VALID') + bconv1\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    h1_flat = tf.reshape(h1,[-1,5408])\n",
    "    y_out = tf.matmul(h1_flat,W1) + b1\n",
    "    return y_out\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define SGD optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(2e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow supports many other layer types, loss functions, and optimizers - you will experiment with these next. Here's the official API documentation for these (if any of the parameters used above were unclear, this resource will also be helpful). \n",
    "\n",
    "* Layers, Activations, Loss functions : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizers: https://www.tensorflow.org/api_guides/python/train#Optimizers\n",
    "* BatchNorm: https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm. Note that there are few other implementations of batch normalization layers, e.g., [link 1](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm), [link 2](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on one epoch\n",
    "Define the function to train a model as following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    \n",
    "    training_now = (training is not None)\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss, correct_prediction, accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    # keep track of losses\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        # shuffle indicies\n",
    "        np.random.shuffle(train_indicies)\n",
    "        # keep track of accuracy\n",
    "        correct = 0\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            corr = np.array(corr).astype(np.float32)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        \n",
    "    if plot_losses:\n",
    "        plt.plot(losses)\n",
    "        plt.grid(True)\n",
    "        plt.title('Epoch {} Loss'.format(e+1))\n",
    "        plt.xlabel('minibatch number')\n",
    "        plt.ylabel('minibatch loss')\n",
    "        plt.show()\n",
    "            \n",
    "    return total_loss,total_correct,losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have defined a graph of operations above, in order to execute TensorFlow Graphs, by feeding them input data and computing the results, we first need to create a `tf.Session` object. A session encapsulates the control and state of the TensorFlow runtime. For more information, see the TensorFlow [Getting started](https://www.tensorflow.org/get_started/get_started) guide.\n",
    "\n",
    "Optionally we can also specify a device context such as `/cpu:0` or `/gpu:0`. For documentation on this behavior see [this TensorFlow guide](https://www.tensorflow.org/tutorials/using_gpu). Generally, if your machine has GPU available (with all required drivers) and you install Tensorflow GPU version, Tensorflow will automatically select GPU as the primary device. Otherwise, CPU will be selected as the primary device.\n",
    "\n",
    "You should see a validation loss of around 1 and an accuracy of 0.2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 9.76 and accuracy of 0.14\n",
      "Iteration 100: with minibatch training loss = 1.9 and accuracy of 0.14\n",
      "Iteration 200: with minibatch training loss = 1.65 and accuracy of 0.17\n",
      "Iteration 300: with minibatch training loss = 1.67 and accuracy of 0.17\n",
      "Iteration 400: with minibatch training loss = 1.19 and accuracy of 0.2\n",
      "Iteration 500: with minibatch training loss = 0.929 and accuracy of 0.38\n",
      "Iteration 600: with minibatch training loss = 0.983 and accuracy of 0.33\n",
      "Iteration 700: with minibatch training loss = 1.12 and accuracy of 0.28\n",
      "Epoch 1, Overall loss = 1.52 and accuracy of 0.203\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f828131f940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch 1, Overall loss = 1.08 and accuracy of 0.233\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step,plot_losses=True)\n",
    "    print('Validation')\n",
    "    run_model(sess,y_out,mean_loss,X_val,y_val,1,64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.4158169454846322&quot;).pbtxt = 'node {\\n  name: &quot;Placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 32\\n        }\\n        dim {\\n          size: 32\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder_1&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder_2&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000 \\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.05914847552776337\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.05914847552776337\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/max&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/mul&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 7\\n        }\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 32\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Wconv1&quot;\\n  input: &quot;Wconv1/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Wconv1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Wconv1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 32\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3061862289905548\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3061862289905548\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/max&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/mul&quot;\\n  input: &quot;bconv1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 32\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;bconv1&quot;\\n  input: &quot;bconv1/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bconv1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;bconv1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot; \\\\025\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.03327791765332222\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.03327791765332222\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;W1/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;W1/Initializer/random_uniform/max&quot;\\n  input: &quot;W1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;W1/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;W1/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;W1/Initializer/random_uniform/mul&quot;\\n  input: &quot;W1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5408\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;W1&quot;\\n  input: &quot;W1/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;W1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 10\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.547722578048706\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.547722578048706\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;b1/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;b1/Initializer/random_uniform/max&quot;\\n  input: &quot;b1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;b1/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;b1/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;b1/Initializer/random_uniform/mul&quot;\\n  input: &quot;b1/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;b1&quot;\\n  input: &quot;b1/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;b1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Wconv1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dilations&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Conv2D&quot;\\n  input: &quot;bconv1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377 \\\\025\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Relu&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;W1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;MatMul&quot;\\n  input: &quot;b1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;one_hot/on_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;one_hot/off_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;one_hot/depth&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 10\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;one_hot&quot;\\n  op: &quot;OneHot&quot;\\n  input: &quot;Placeholder_1&quot;\\n  input: &quot;one_hot/depth&quot;\\n  input: &quot;one_hot/on_value&quot;\\n  input: &quot;one_hot/off_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;TI&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: -1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;one_hot&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;hinge_loss/ones_like/Shape&quot;\\n  input: &quot;hinge_loss/ones_like/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/mul/x&quot;\\n  input: &quot;one_hot&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hinge_loss/mul&quot;\\n  input: &quot;hinge_loss/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/Sub&quot;\\n  input: &quot;add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hinge_loss/ones_like&quot;\\n  input: &quot;hinge_loss/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;hinge_loss/Sub_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/weights&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/weights/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/weights/rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/values/shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/values/rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  op: &quot;NoOp&quot;\\n}\\nnode {\\n  name: &quot;hinge_loss/ToFloat_3/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  input: &quot;hinge_loss/ToFloat_3/x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;hinge_loss/Mul_1&quot;\\n  input: &quot;hinge_loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/Equal/y&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;hinge_loss/ToFloat_3/x&quot;\\n  input: &quot;hinge_loss/num_present/Equal/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/ones_like/Shape&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/ones_like/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/ones_like&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;hinge_loss/num_present/ones_like/Shape&quot;\\n  input: &quot;hinge_loss/num_present/ones_like/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/num_present/Equal&quot;\\n  input: &quot;hinge_loss/num_present/zeros_like&quot;\\n  input: &quot;hinge_loss/num_present/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/assert_broadcastable/weights/shape&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/assert_broadcastable/weights/rank&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/assert_broadcastable/values/shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/assert_broadcastable/values/rank&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/assert_broadcastable/static_scalar_check_success&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/ones_like/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  input: &quot;^hinge_loss/num_present/broadcast_weights/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/ones_like/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  input: &quot;^hinge_loss/num_present/broadcast_weights/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights/ones_like&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights/ones_like/Shape&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights/ones_like/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/broadcast_weights&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/num_present/Select&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/num_present&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights&quot;\\n  input: &quot;hinge_loss/num_present/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Const_1&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;hinge_loss/Sum&quot;\\n  input: &quot;hinge_loss/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Greater/y&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Greater&quot;\\n  op: &quot;Greater&quot;\\n  input: &quot;hinge_loss/num_present&quot;\\n  input: &quot;hinge_loss/Greater/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Equal/y&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;hinge_loss/num_present&quot;\\n  input: &quot;hinge_loss/Equal/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like_1/Shape&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like_1/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/ones_like_1&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;hinge_loss/ones_like_1/Shape&quot;\\n  input: &quot;hinge_loss/ones_like_1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Equal&quot;\\n  input: &quot;hinge_loss/ones_like_1&quot;\\n  input: &quot;hinge_loss/num_present&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/div&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;hinge_loss/Sum_1&quot;\\n  input: &quot;hinge_loss/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^hinge_loss/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hinge_loss/value&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Greater&quot;\\n  input: &quot;hinge_loss/div&quot;\\n  input: &quot;hinge_loss/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;hinge_loss/value&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/Mean_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Mean_grad/Reshape&quot;\\n  input: &quot;gradients/Mean_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/Mean_grad/Tile&quot;\\n  input: &quot;gradients/Mean_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Greater&quot;\\n  input: &quot;gradients/Mean_grad/truediv&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Greater&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/zeros_like&quot;\\n  input: &quot;gradients/Mean_grad/truediv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/value_grad/Select&quot;\\n  input: &quot;^gradients/hinge_loss/value_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/Select&quot;\\n  input: &quot;^gradients/hinge_loss/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/value_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/value_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/Select_1&quot;\\n  input: &quot;^gradients/hinge_loss/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/value_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Shape&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/RealDiv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/tuple/control_dependency&quot;\\n  input: &quot;hinge_loss/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/RealDiv&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Sum&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;hinge_loss/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/RealDiv_1&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Neg&quot;\\n  input: &quot;hinge_loss/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/RealDiv_2&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/RealDiv_1&quot;\\n  input: &quot;hinge_loss/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/hinge_loss/value_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/RealDiv_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/mul&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Sum_1&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/div_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/div_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/div_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/div_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hinge_loss/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/div_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_1_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/hinge_loss/Sum_1_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_1_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/hinge_loss/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Sum_1_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Equal&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/hinge_loss/Select_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;hinge_loss/Equal&quot;\\n  input: &quot;gradients/hinge_loss/Select_grad/zeros_like&quot;\\n  input: &quot;gradients/hinge_loss/div_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/Select_grad/Select&quot;\\n  input: &quot;^gradients/hinge_loss/Select_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Select_grad/Select&quot;\\n  input: &quot;^gradients/hinge_loss/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Select_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Select_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Select_grad/Select_1&quot;\\n  input: &quot;^gradients/hinge_loss/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Select_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Sum_1_grad/Tile&quot;\\n  input: &quot;gradients/hinge_loss/Sum_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/hinge_loss/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Sum_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Shape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/hinge_loss/Sum_grad/Tile&quot;\\n  input: &quot;hinge_loss/ToFloat_3/x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/mul&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  input: &quot;gradients/hinge_loss/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Select_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/hinge_loss/num_present_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/hinge_loss/num_present_grad/Reshape&quot;\\n  input: &quot;gradients/hinge_loss/num_present_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/hinge_loss/num_present_grad/Tile&quot;\\n  input: &quot;hinge_loss/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/mul&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Sum&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/num_present/Select&quot;\\n  input: &quot;gradients/hinge_loss/num_present_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/mul_1&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Sum_1&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/num_present/broadcast_weights_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/num_present/broadcast_weights/ones_like_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/hinge_loss/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/hinge_loss/Mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;hinge_loss/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Shape&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Relu_grad/ReluGrad&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Sum&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Relu_grad/ReluGrad&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Neg&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/Sub_1_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Sub_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Sub_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Sub_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Sub_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hinge_loss/Sub_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Sub_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;hinge_loss/Sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Shape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/mul&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hinge_loss/Sub&quot;\\n  input: &quot;gradients/hinge_loss/Sub_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/mul_1&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Sum_1&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/hinge_loss/Mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/hinge_loss/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/hinge_loss/Mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;MatMul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 10\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/add_1_grad/Shape&quot;\\n  input: &quot;gradients/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/add_1_grad/Sum&quot;\\n  input: &quot;gradients/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/hinge_loss/Mul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;W1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;gradients/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/Reshape_grad/Reshape&quot;\\n  input: &quot;Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 32\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/add_grad/Shape&quot;\\n  input: &quot;gradients/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Relu_grad/ReluGrad&quot;\\n  input: &quot;gradients/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/add_grad/Sum&quot;\\n  input: &quot;gradients/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Relu_grad/ReluGrad&quot;\\n  input: &quot;gradients/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/add_grad/Sum_1&quot;\\n  input: &quot;gradients/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/add_grad/Reshape&quot;\\n  input: &quot;^gradients/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/add_grad/Reshape&quot;\\n  input: &quot;^gradients/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Wconv1/read&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\007\\\\000\\\\000\\\\000\\\\007\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000 \\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Conv2D_grad/ShapeN&quot;\\n  input: &quot;Wconv1/read&quot;\\n  input: &quot;gradients/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dilations&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;gradients/Conv2D_grad/Const&quot;\\n  input: &quot;gradients/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dilations&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;VALID&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.00019999999494757503\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_Wconv1/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;Wconv1&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/Conv2D_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Wconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_bconv1/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;bconv1&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/add_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bconv1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_W1/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;W1&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@W1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_b1/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;b1&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@b1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_Wconv1/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_bconv1/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_W1/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_b1/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Wconv1/Assign&quot;\\n  input: &quot;^bconv1/Assign&quot;\\n  input: &quot;^W1/Assign&quot;\\n  input: &quot;^b1/Assign&quot;\\n}\\nnode {\\n  name: &quot;ArgMax/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;add_1&quot;\\n  input: &quot;ArgMax/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;ArgMax&quot;\\n  input: &quot;Placeholder_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Mean_1&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Cast&quot;\\n  input: &quot;Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax_1/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax_1&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;add_1&quot;\\n  input: &quot;ArgMax_1/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;output_type&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Equal_1&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;ArgMax_1&quot;\\n  input: &quot;Placeholder_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cast_1&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;Equal_1&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Mean_2&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Cast_1&quot;\\n  input: &quot;Const_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.4158169454846322&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard for Visualization\n",
    "\n",
    "Tensorflow provides a very useful tool: Tensorboard. This is very helpful to visualize the training loss, accuray, filters,...\n",
    "\n",
    "Here is a good video about Tensorboard: https://www.youtube.com/watch?v=eBbEDRsCmv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def simple_model(X,y):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[5408, 10])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[10])\n",
    "    variable_summaries(Wconv1)\n",
    "    variable_summaries(W1)\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding='VALID') + bconv1\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    h1_flat = tf.reshape(h1,[-1,5408])\n",
    "    y_out = tf.matmul(h1_flat,W1) + b1\n",
    "    return y_out\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define SGD optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(2e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with_tensorboard(session, predict, loss_value, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, tensorboard_writer=None):\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    \n",
    "    training_now = (training is not None)\n",
    "    \n",
    "    tf.summary.scalar(\"cost\", loss_value)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    # keep track of losses\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        # shuffle indicies\n",
    "        np.random.shuffle(train_indicies)\n",
    "        # keep track of accuracy\n",
    "        correct = 0\n",
    "        # make sure we iterate over the dataset once\n",
    "        batch_count = int(math.ceil(Xd.shape[0]/batch_size))\n",
    "        for i in range(batch_count):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            \n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            if training_now:\n",
    "                _, summary = session.run([training, summary_op],feed_dict=feed_dict)\n",
    "                # write log\n",
    "                tensorboard_writer.add_summary(summary, e * batch_count + i)\n",
    "            else:\n",
    "                summary = session.run(summary_op, feed_dict=feed_dict)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD optimizer\n",
      "Training\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=logs/train \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# define SGD optimizer\n",
    "print('SGD optimizer')\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter('logs/train', graph=tf.get_default_graph())\n",
    "    #with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    run_model_with_tensorboard(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step,\n",
    "                               tensorboard_writer=train_writer)\n",
    "\n",
    "# tensorboard --logdir=logs/train\n",
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=logs/train \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "\n",
    "# NOTE: In Window, you may not able to run the Tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "\n",
    "You are going to see [ADAM optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) being used. Now, we will compare the training loss curves of a model with SGD and ADAM optimizers.\n",
    "\n",
    "You can try other optimizers, e.g., [SGD+Momentum](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer), [RMSprop](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer), [Adagrad](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer), [Adadelta](https://www.tensorflow.org/api_docs/python/tf/train/AdadeltaOptimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD optimizer\n",
      "Training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_out_deep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d957c2718602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_out_deep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_out_deep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_out_deep' is not defined"
     ]
    }
   ],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define SGD optimizer\n",
    "print('SGD optimizer')\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    _,_, sgd_losses = run_model(sess,y_out_deep,mean_loss,X_train,y_train,1,64,100,train_step,False)\n",
    "    print('Validation')\n",
    "    run_model(sess,y_out_deep,mean_loss,X_val,y_val,1,64)\n",
    "\n",
    "print(\"==========================================================\\n\")\n",
    "# define Adam optimizer\n",
    "print('ADAM optimizer')\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    _,_, adam_losses = run_model(sess,y_out_deep,mean_loss,X_train,y_train,1,64,100,train_step,False)\n",
    "    print('Validation')\n",
    "    run_model(sess,y_out_deep,mean_loss,X_val,y_val,1,64)\n",
    "    \n",
    "plt.plot(sgd_losses, label='SGD')\n",
    "plt.plot(adam_losses, label='ADAM')\n",
    "# plt.plot(adam_losses_batchnorm, label='ADAM+BatchNorm')\n",
    "# plt.ylim( (0, 100) ) \n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Epoch 1 Loss')\n",
    "plt.xlabel('minibatch number')\n",
    "plt.ylabel('minibatch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go Deeper\n",
    "\n",
    "In the previous exercises, you are required to implement different functions, e.g., affine, relu, conv2d, ... which serve as basic module to build a Deep Neuron Network. Similarly, we provide the basic modules for Tensorflow in `libs/tf_layers.py`.\n",
    "\n",
    "**NOTE:** In this exercise, you are welcome to change the block functions in `libs/tf_layers.py` to fit your needs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a deep network\n",
    "def deep_model(X, y, batchnorm=False, name=None):\n",
    "    output = Conv2D(X, 3, 7, 8, name=name+'_conv1')\n",
    "    output = tf.nn.relu(output, name=name+'_relu1')\n",
    "    if batchnorm:\n",
    "        output = BatchNormalization(output, True, name=name+'_BN1')\n",
    "    output = Conv2D(output, 8, 7, 8, name=name+'_conv2')\n",
    "    output = tf.nn.relu(output, name=name+'_relu2')\n",
    "    if batchnorm:\n",
    "        output = BatchNormalization(output, True, name=name+'_BN2')\n",
    "    output = MaxPooling2D(output, name=name+'_maxpool1')\n",
    "    output = Conv2D(output, 8, 7, 16, name=name+'_conv3')\n",
    "    output = tf.nn.relu(output, name=name+'_relu3')\n",
    "    if batchnorm:\n",
    "        output = BatchNormalization(output, True, name=name+'_BN3')\n",
    "    output = Conv2D(output, 16, 7, 16, name=name+'_conv4')\n",
    "    \n",
    "    # Here is another way of defining a name for a layer\n",
    "    with tf.variable_scope(name+'_relu4'):\n",
    "#         output = tf.nn.relu(output, name=name+'_relu4')\n",
    "        output = tf.nn.relu(output)\n",
    "    if batchnorm:\n",
    "        output = BatchNormalization(output, True, name=name+'_BN4')\n",
    "    output = MaxPooling2D(output, name=name+'_maxpool2')\n",
    "    output = tf.reshape(output, [-1, 16*8*8], name=name+'_flatten')\n",
    "    output = FullyConnected(output, 16*8*8, 100, name=name+'_fc1')\n",
    "    output = tf.nn.relu(output, name=name+'_relu5')\n",
    "    output = FullyConnected(output, 100, 100, name=name+'_fc2')\n",
    "    output = tf.nn.relu(output, name=name+'_relu6')\n",
    "    output = FullyConnected(output, 100, 10, name=name+'_fc3')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see the benefit of using **batch normalization** layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out_deep2 = deep_model(X,y, True, name='deep2')\n",
    "# define our loss\n",
    "total_loss_deep2 = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out_deep2)\n",
    "mean_loss = tf.reduce_mean(total_loss_deep2)\n",
    "\n",
    "print(\"==========================================================\\n\")\n",
    "# define Adam optimizer\n",
    "print('ADAM optimizer')\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) # select optimizer and set learning rate\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    _,_, adam_losses_batchnorm = run_model(sess,y_out_deep2,mean_loss,X_train,y_train,1,64,100,train_step,False)\n",
    "    print('Validation')\n",
    "    run_model(sess,y_out_deep2,mean_loss,X_val,y_val,1,64)\n",
    "    \n",
    "plt.plot(sgd_losses, label='SGD')\n",
    "plt.plot(adam_losses, label='ADAM')\n",
    "plt.plot(adam_losses_batchnorm, label='ADAM+BatchNorm')\n",
    "plt.ylim( (0, 100) ) \n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Epoch 1 Loss')\n",
    "plt.xlabel('minibatch number')\n",
    "plt.ylabel('minibatch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More epochs\n",
    "Train the model with more epochs to see how good performance it can achieve.\n",
    "\n",
    "**NOTE:** If you run this on a CPU, it will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out_deep2 = deep_model(X,y, True, name='deep2')\n",
    "# define our loss\n",
    "total_loss_deep2 = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out_deep2)\n",
    "mean_loss = tf.reduce_mean(total_loss_deep2)\n",
    "\n",
    "print(\"==========================================================\")\n",
    "# define Adam optimizer\n",
    "print('ADAM optimizer')\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) # select optimizer and set learning rate\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    _,_, adam_losses_batchnorm = run_model(sess,y_out_deep2,mean_loss,X_train,y_train,5,64,100,train_step,True)\n",
    "    print('Validation')\n",
    "    run_model(sess,y_out_deep2,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train a _GREAT_ model on CIFAR-10!\n",
    "\n",
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; bigger filters captures more information but smaller filters may be more computationally efficient.\n",
    "- **Number of filters**: Above we used 32 (or less) filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Use TensorFlow Scope**: Use TensorFlow scope and/or [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers) to make it easier to write deeper networks. See [this tutorial](https://www.tensorflow.org/tutorials/layers) for how to use `tf.layers`. \n",
    "- **Use Learning Rate Decay**: [As the notes point out](http://cs231n.github.io/neural-networks-3/#anneal), decaying the learning rate might help the model converge. Feel free to decay every epoch, when loss doesn't change over an entire epoch, or any other heuristic you find appropriate. See the [Tensorflow documentation](https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate) for learning rate decay.\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use [Dropout as in the TensorFlow MNIST tutorial](https://www.tensorflow.org/get_started/mnist/pros).\n",
    "\n",
    "**NOTE:**\n",
    "* In this exercise, you are welcome to change the block functions in `libs/tf_layers.py` to fit your needs the best.\n",
    "* Softmax cross-entropy loss: [tf.losses.softmax_cross_entropy](https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/losses/softmax_cross_entropy)\n",
    "* SVM loss: [tf.losses.hinge_loss](https://www.tensorflow.org/api_docs/python/tf/losses/hinge_loss)\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should **see improvement within a few hundred iterations.**\n",
    "- Remember the **coarse-to-fine** approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should **use the validation set for hyperparameter search**, and we'll save the test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at **>= 60% accuracy on the validation set**. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training and validation set accuracies for your final trained network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to play with this cell\n",
    "# You can implement the model in a seperate python file.\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    pass\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "mean_loss = None\n",
    "optimizer = None\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to play with this cell\n",
    "# This default code creates a session\n",
    "# and trains your model for 10 epochs\n",
    "# then prints the validation set accuracy\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,10,64,100,train_step,True)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your model here, and make sure \n",
    "# the output of this cell is the accuracy\n",
    "# of your best model on the training and val sets\n",
    "# We're looking for >= 60% accuracy on Validation\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set - DO THIS ONLY ONCE\n",
    "Now that we've gotten a result that we're happy with, we test our final model on the test set. This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit Description\n",
    "Briefly describe what you did here.\n",
    "\n",
    "In this cell you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tell us here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
